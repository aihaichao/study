{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark sql study"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=sc.parallelize([\"pan\",\"i like pan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pan'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print i*i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#导入spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext,Row"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#当不能引入hive依赖时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext,Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlctx=sqlContextl(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hivectx=HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入门"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 创建基本的sparksession，只需要使用sparksession.builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "根据JSON文件的内容创建一个DataFrame："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "路径 本次是项目所在的路径下开始找 ，（或者 启动服务的目录开始找） \n",
    "file:/root/Downloads/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(\"../../examples/src/main/resources/people.json\")\n",
    "#df=spark.read.json(\"./examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+\n",
      "|   name| age|(age + 1)|\n",
      "+-------+----+---------+\n",
      "|Michael|null|     null|\n",
      "|   Andy|  30|       31|\n",
      "| Justin|  19|       20|\n",
      "+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'],df['age'],df['age']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age']>21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('age').count().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SparkSession上的sql函数使应用程序能够以编程方式运行SQL查询，并将结果作为DataFrame返回。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Register the DataFrame as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF=spark.sql(\"select * from people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Global temporary view is cross-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集(Dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inferring the Schema Using Reflection   使用反射推理schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines=sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts=lines.map(lambda l:l.split(\",\"))\n",
    "people=parts.map(lambda p: Row(name=p[0],age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople =spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "#spark.sql(\"select * from people\").show()\n",
    "#SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers=spark.sql(\"select name from people where age>=13 and age <=19\")\n",
    "teenagers.show()\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "\n",
    "teenNames=teenagers.rdd.map(lambda p: \"Name: \" +p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "\n",
    "# for name in teenagers:\n",
    "#     print(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Programmatically Specifying the Schema 已编程方式指定schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "lines=sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts=lines.map(lambda l:l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people=parts.map(lambda p:(p[0],p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString=\"name age\"\n",
    "\n",
    "fields=[StructField(field_name,StringType(),True) for field_name in schemaString.split()]\n",
    "schema=StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople=spark.createDataFrame(people,schema)\n",
    "\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "#results=spark.sql(\"select name from people\")\n",
    "results=spark.sql(\"select * from people\")\n",
    "results.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\",\"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "直接在文件上运行SQL     \"   ``  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from parquet.`examples/src/main/resources/users.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "保存到持久表    saveAsTable将物化saveAsTable的内容并创建指向Hive Metastore中的数据的指针"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "+------------------+----+\n",
      "|            adress|name|\n",
      "+------------------+----+\n",
      "|[Beijing,xizhimen]| yin|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "path=\"examples/src/main/resources/people.json\"\n",
    "peopleDF=spark.read.json(path)\n",
    "\n",
    "peopleDF.printSchema()\n",
    "\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "teenagerNamesDF=spark.sql(\"select name from people where age between 13 and 19\")\n",
    "teenagerNamesDF.show()\n",
    "\n",
    "jsonString=['{\"name\":\"yin\",\"adress\":{\"city\":\"Beijing\",\"state\":\"xizhimen\"}}']\n",
    "otehpeopleRDD=sc.parallelize(jsonString)\n",
    "otherpeople=spark.read.json(otehpeopleRDD)\n",
    "otherpeople.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JDBC To Other Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "# Loading data from a JDBC source\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Saving data to a JDBC source\n",
    "jdbcDF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .save()\n",
    "\n",
    "jdbcDF2.write \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分布式SQL引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thrift JDBC / ODBC服务器：\n",
    "./bin/beeline\n",
    "beeline> !connect jdbc:hive2://localhost:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext\n",
    "path=\"../../spark-test.csv\"\n",
    "financeDF=spark.read.csv(path,header=True)\n",
    "# financeDF.printSchema()\n",
    "\n",
    "financeDF.createGlobalTempView(\"finance\")\n",
    "# teenagerNamesDF=spark.sql(\"select * from global_temp.finance limit 10 \")\n",
    "# teenagerNamesDF.cache(\"fa\")\n",
    "# teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--中文  spark.sql(\"\"\"select date as `是` from global_temp.finance limit 10\"\"\")    --使用限定符 来写中文别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teenagerNamesDF=spark.sql(\"\"\"select date as shi from global_temp.finance limit 10\"\"\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+\n",
      "|    date|data_type|               收入|\n",
      "+--------+---------+-----------------+\n",
      "|2016/4/1|       实际|       1310081.17|\n",
      "|2016/5/1|       实际|5463629.550000001|\n",
      "|2016/6/1|       实际|             40.0|\n",
      "+--------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagerNamesDF=spark.sql(\"\"\" select date  ,substring(data_type,5) as data_type,\n",
    "sum( case when subject_type='收入' then money else 0 end )  `收入`\n",
    "from global_temp.finance \n",
    "group by date,data_type\n",
    "order by date,data_type\n",
    "  \"\"\")\n",
    "teenagerNamesDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+\n",
      "|    date|data_type|               收入|\n",
      "+--------+---------+-----------------+\n",
      "|2016/4/1|       实际|       1310081.17|\n",
      "|2016/5/1|       实际|5463629.550000001|\n",
      "|2016/6/1|       实际|             40.0|\n",
      "+--------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagerNamesDF.createGlobalTempView(\"finance_basic\")\n",
    "fbDF=spark.sql(\"\"\"select * from global_temp.finance_basic\"\"\")\n",
    "fbDF.show()\n",
    "\n",
    "# teenagerNamesDF.createOrReplaceTempView(\"finance_basic\")\n",
    "# fbDF=spark.sql(\"\"\"select * from finance_basic\"\"\")\n",
    "# fbDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+\n",
      "|    date|data_type|        收入|\n",
      "+--------+---------+----------+\n",
      "|2016/4/1|       实际|1310081.17|\n",
      "+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fbDF=spark.sql(\"\"\"select * from global_temp.finance_basic limit 1\"\"\")\n",
    "fbDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 9))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u\"Table or view not found: `global_temp`.`finace`; line 23 pos 5;\\n'GlobalLimit 10\\n+- 'LocalLimit 10\\n   +- 'Aggregate ['date_part(y, 'DATEADD(mon, -3, 'date)), 'to_char('date, yyyy-mm), 'to_char('add_months('date, -12), yyyy-mm), 'date, 'area, 'data_type, 'substring('data_type, 5), CASE WHEN 'area IN (\\u897f\\u5eb7,\\u534e\\u68c0,\\u5143\\u5316\\u533b\\u7597,\\u5065\\u7ef4\\u7ba1\\u7406,\\u81fb\\u666f,\\u9999\\u6e2f,BVI) THEN \\u975e\\u4e00\\u4f53\\u5316 ELSE \\u4e00\\u4f53\\u5316 END], ['date_part(y, 'DATEADD(mon, -3, 'date)) AS \\u8d22\\u5e74#130, 'to_char('date, yyyy-mm) AS \\u6708\\u4efd#131, 'to_char('add_months('date, -12), yyyy-mm) AS \\u53bb\\u5e74\\u540c\\u671f\\u6708\\u4efd#132, 'date AS \\u65e5\\u671f#133, 'area AS \\u533a\\u57df#134, 'data_type AS \\u6570\\u636e\\u7c7b\\u578b#135, 'substring('data_type, 5) AS \\u6570\\u636e\\u7c7b\\u578b2#136, 'sum(CASE WHEN ('subject_type = \\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u5168\\u6536\\u5165#137, 'sum(CASE WHEN (('subject_type = \\u6536\\u5165) && ('subject_code = 60011108)) THEN 'money ELSE 0 END) AS \\u9f7f\\u79d1\\u53cc\\u7b97#138, 'sum(CASE WHEN ('subject_detail_type = \\u4f53\\u68c0\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u4f53\\u68c0\\u6536\\u5165#139, 'sum(CASE WHEN (('subject_code = 64010201) || ('subject_code = 64010202)) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c1#140, 'sum(CASE WHEN ('subject_detail_type = \\u75be\\u75c5\\u68c0\\u6d4b\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u75be\\u75c5\\u68c0\\u6d4b\\u6536\\u5165#141, 'sum(CASE WHEN ('subject_detail_type = \\u9f7f\\u79d1\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u9f7f\\u79d1\\u6536\\u5165#142, 'sum(CASE WHEN ('subject_code = 64010221) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c3#143, 'sum(CASE WHEN ('subject_detail_type = \\u95e8\\u8bca\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u95e8\\u8bca\\u6536\\u5165#144, 'sum(CASE WHEN ('subject_code = 64010222) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c4#145, 'sum(CASE WHEN ('subject_detail_type = \\u533b\\u7597\\u7ba1\\u7406\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u533b\\u7597\\u7ba1\\u7406\\u6536\\u5165#146, 'sum(CASE WHEN ('subject_code = 64010211) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c5#147, 'sum(CASE WHEN ('subject_detail_type = \\u9500\\u552e\\u5546\\u54c1\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u9500\\u552e\\u5546\\u54c1\\u6536\\u5165#148, 'sum(CASE WHEN ('subject_detail_type = \\u5176\\u4ed6\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u5176\\u4ed6\\u6536\\u5165#149, 'sum(CASE WHEN ('subject_detail_type = \\u843d\\u5173\\u8054\\u6210\\u672c) THEN 'money ELSE 0 END) AS \\u843d\\u5173\\u8054\\u6210\\u672c#150, 'sum(CASE WHEN ('subject_type = \\u6210\\u672c\\u8d39\\u7528) THEN 'money ELSE 0 END) AS \\u6210\\u672c\\u8d39\\u7528#151, ('nvl('sum(CASE WHEN ('subject_type = \\u6536\\u5165) THEN 'money ELSE 0 END), 0) - 'nvl('sum(CASE WHEN (('subject_type = \\u6536\\u5165) && ('subject_code = 60011108)) THEN 'money ELSE 0 END), 0)) AS \\u6536\\u5165#152, CASE WHEN 'area IN (\\u897f\\u5eb7,\\u534e\\u68c0,\\u5143\\u5316\\u533b\\u7597,\\u5065\\u7ef4\\u7ba1\\u7406,\\u81fb\\u666f,\\u9999\\u6e2f,BVI) THEN \\u975e\\u4e00\\u4f53\\u5316 ELSE \\u4e00\\u4f53\\u5316 END AS \\u662f\\u5426\\u4e00\\u4f53\\u5316#153]\\n      +- 'UnresolvedRelation `global_temp`.`finace`\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-123e18d5dd96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdate\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0marea\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcase\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0marea\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'西康'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'华检'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'元化医疗'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'健维管理'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'臻景'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'香港'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'BVI'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mthen\u001b[0m \u001b[0;34m'非一体化'\u001b[0m  \u001b[0;32melse\u001b[0m \u001b[0;34m'一体化'\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m limit 10 \"\"\")\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/Downloads/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Table or view not found: `global_temp`.`finace`; line 23 pos 5;\\n'GlobalLimit 10\\n+- 'LocalLimit 10\\n   +- 'Aggregate ['date_part(y, 'DATEADD(mon, -3, 'date)), 'to_char('date, yyyy-mm), 'to_char('add_months('date, -12), yyyy-mm), 'date, 'area, 'data_type, 'substring('data_type, 5), CASE WHEN 'area IN (\\u897f\\u5eb7,\\u534e\\u68c0,\\u5143\\u5316\\u533b\\u7597,\\u5065\\u7ef4\\u7ba1\\u7406,\\u81fb\\u666f,\\u9999\\u6e2f,BVI) THEN \\u975e\\u4e00\\u4f53\\u5316 ELSE \\u4e00\\u4f53\\u5316 END], ['date_part(y, 'DATEADD(mon, -3, 'date)) AS \\u8d22\\u5e74#130, 'to_char('date, yyyy-mm) AS \\u6708\\u4efd#131, 'to_char('add_months('date, -12), yyyy-mm) AS \\u53bb\\u5e74\\u540c\\u671f\\u6708\\u4efd#132, 'date AS \\u65e5\\u671f#133, 'area AS \\u533a\\u57df#134, 'data_type AS \\u6570\\u636e\\u7c7b\\u578b#135, 'substring('data_type, 5) AS \\u6570\\u636e\\u7c7b\\u578b2#136, 'sum(CASE WHEN ('subject_type = \\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u5168\\u6536\\u5165#137, 'sum(CASE WHEN (('subject_type = \\u6536\\u5165) && ('subject_code = 60011108)) THEN 'money ELSE 0 END) AS \\u9f7f\\u79d1\\u53cc\\u7b97#138, 'sum(CASE WHEN ('subject_detail_type = \\u4f53\\u68c0\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u4f53\\u68c0\\u6536\\u5165#139, 'sum(CASE WHEN (('subject_code = 64010201) || ('subject_code = 64010202)) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c1#140, 'sum(CASE WHEN ('subject_detail_type = \\u75be\\u75c5\\u68c0\\u6d4b\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u75be\\u75c5\\u68c0\\u6d4b\\u6536\\u5165#141, 'sum(CASE WHEN ('subject_detail_type = \\u9f7f\\u79d1\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u9f7f\\u79d1\\u6536\\u5165#142, 'sum(CASE WHEN ('subject_code = 64010221) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c3#143, 'sum(CASE WHEN ('subject_detail_type = \\u95e8\\u8bca\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u95e8\\u8bca\\u6536\\u5165#144, 'sum(CASE WHEN ('subject_code = 64010222) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c4#145, 'sum(CASE WHEN ('subject_detail_type = \\u533b\\u7597\\u7ba1\\u7406\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u533b\\u7597\\u7ba1\\u7406\\u6536\\u5165#146, 'sum(CASE WHEN ('subject_code = 64010211) THEN 'money ELSE 0 END) AS \\u53d8\\u52a8\\u6210\\u672c5#147, 'sum(CASE WHEN ('subject_detail_type = \\u9500\\u552e\\u5546\\u54c1\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u9500\\u552e\\u5546\\u54c1\\u6536\\u5165#148, 'sum(CASE WHEN ('subject_detail_type = \\u5176\\u4ed6\\u6536\\u5165) THEN 'money ELSE 0 END) AS \\u5176\\u4ed6\\u6536\\u5165#149, 'sum(CASE WHEN ('subject_detail_type = \\u843d\\u5173\\u8054\\u6210\\u672c) THEN 'money ELSE 0 END) AS \\u843d\\u5173\\u8054\\u6210\\u672c#150, 'sum(CASE WHEN ('subject_type = \\u6210\\u672c\\u8d39\\u7528) THEN 'money ELSE 0 END) AS \\u6210\\u672c\\u8d39\\u7528#151, ('nvl('sum(CASE WHEN ('subject_type = \\u6536\\u5165) THEN 'money ELSE 0 END), 0) - 'nvl('sum(CASE WHEN (('subject_type = \\u6536\\u5165) && ('subject_code = 60011108)) THEN 'money ELSE 0 END), 0)) AS \\u6536\\u5165#152, CASE WHEN 'area IN (\\u897f\\u5eb7,\\u534e\\u68c0,\\u5143\\u5316\\u533b\\u7597,\\u5065\\u7ef4\\u7ba1\\u7406,\\u81fb\\u666f,\\u9999\\u6e2f,BVI) THEN \\u975e\\u4e00\\u4f53\\u5316 ELSE \\u4e00\\u4f53\\u5316 END AS \\u662f\\u5426\\u4e00\\u4f53\\u5316#153]\\n      +- 'UnresolvedRelation `global_temp`.`finace`\\n\""
     ]
    }
   ],
   "source": [
    "teenagerNamesDF=spark.sql(\"\"\"  select date_part('y',DATEADD('mon',-3,date)) as `财年`,to_char(date,'yyyy-mm') as `月份`\n",
    ",to_char(add_months(date,-12),'yyyy-mm') as `去年同期月份`,\n",
    "date `日期`,area `区域`,data_type `数据类型`,\n",
    "substring(data_type,5) `数据类型2`,\n",
    "sum( case when subject_type='收入' then money else 0 end ) as `全收入`,\n",
    "sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ) as `齿科双算`, -- 60011108收入 \n",
    "sum( case when subject_detail_type='体检收入' then money else 0 end ) as `体检收入`,\n",
    "sum( case when subject_code='64010201' or subject_code='64010202' then money else 0 end ) as  `变动成本1` ,   --64010201 and 64010202\n",
    "sum( case when subject_detail_type='疾病检测收入' then money else 0 end ) as `疾病检测收入`,\n",
    "sum( case when subject_detail_type='齿科收入' then money else 0 end ) as `齿科收入`,\n",
    "sum( case when subject_code='64010221' then money else 0 end ) as `变动成本3`   ,  --64010221\n",
    "sum( case when subject_detail_type='门诊收入' then money else 0 end ) as `门诊收入`,\n",
    "sum( case when subject_code='64010222' then money else 0 end ) as `变动成本4`  , --64010222\n",
    "sum( case when subject_detail_type='医疗管理收入' then money else 0 end ) as `医疗管理收入`,\n",
    "sum( case when subject_code='64010211' then money else 0 end ) as `变动成本5`,   --64010211\n",
    "sum( case when subject_detail_type='销售商品收入' then money else 0 end ) as `销售商品收入`,\n",
    "sum( case when subject_detail_type='其他收入' then money else 0 end ) as `其他收入`,\n",
    "sum( case when subject_detail_type='落关联成本' then money else 0 end ) as `落关联成本`,\n",
    "sum( case when subject_type='成本费用' then money else 0 end ) as `成本费用`, \n",
    "nvl( sum( case when subject_type='收入' then money else 0 end ),0)-nvl(sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ),0) as `收入`,\n",
    "-- nvl( sum( case when subject_type='收入' then money else 0 end ),0)-nvl(sum( case when subject_type='成本费用' then money else 0 end ) ,0) as `税前利润`,\n",
    "case when area in ('西康','华检','元化医疗','健维管理','臻景','香港','BVI') then '非一体化'  else '一体化' end as `是否一体化`\n",
    "from global_temp.finace\n",
    "group by date_part('y',DATEADD('mon',-3,date)),to_char(date,'yyyy-mm'),to_char(add_months(date,-12),'yyyy-mm'),\n",
    "\tdate ,area ,data_type,substring(data_type,5),\n",
    "\tcase when area in ('西康','华检','元化医疗','健维管理','臻景','香港','BVI') then '非一体化'  else '一体化' end\n",
    "limit 10 \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
