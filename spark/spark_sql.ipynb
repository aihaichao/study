{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark sql study"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=sc.parallelize([\"pan\",\"i like pan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pan'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print i*i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#导入spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext,Row"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#当不能引入hive依赖时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext,Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlctx=sqlContextl(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hivectx=HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入门"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 创建基本的sparksession，只需要使用sparksession.builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "根据JSON文件的内容创建一个DataFrame："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "路径 本次是项目所在的路径下开始找 ，（或者 启动服务的目录开始找） \n",
    "file:/root/Downloads/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(\"../../examples/src/main/resources/people.json\")\n",
    "#df=spark.read.json(\"./examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+\n",
      "|   name| age|(age + 1)|\n",
      "+-------+----+---------+\n",
      "|Michael|null|     null|\n",
      "|   Andy|  30|       31|\n",
      "| Justin|  19|       20|\n",
      "+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'],df['age'],df['age']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age']>21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('age').count().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SparkSession上的sql函数使应用程序能够以编程方式运行SQL查询，并将结果作为DataFrame返回。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Register the DataFrame as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF=spark.sql(\"select * from people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Global temporary view is cross-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集(Dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inferring the Schema Using Reflection   使用反射推理schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines=sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts=lines.map(lambda l:l.split(\",\"))\n",
    "people=parts.map(lambda p: Row(name=p[0],age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople =spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "#spark.sql(\"select * from people\").show()\n",
    "#SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers=spark.sql(\"select name from people where age>=13 and age <=19\")\n",
    "teenagers.show()\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "\n",
    "teenNames=teenagers.rdd.map(lambda p: \"Name: \" +p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "\n",
    "# for name in teenagers:\n",
    "#     print(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Programmatically Specifying the Schema 已编程方式指定schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "lines=sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts=lines.map(lambda l:l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people=parts.map(lambda p:(p[0],p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString=\"name age\"\n",
    "\n",
    "fields=[StructField(field_name,StringType(),True) for field_name in schemaString.split()]\n",
    "schema=StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople=spark.createDataFrame(people,schema)\n",
    "\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "#results=spark.sql(\"select name from people\")\n",
    "results=spark.sql(\"select * from people\")\n",
    "results.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\",\"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "直接在文件上运行SQL     \"   ``  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from parquet.`examples/src/main/resources/users.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "保存到持久表    saveAsTable将物化saveAsTable的内容并创建指向Hive Metastore中的数据的指针"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "+------------------+----+\n",
      "|            adress|name|\n",
      "+------------------+----+\n",
      "|[Beijing,xizhimen]| yin|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "path=\"examples/src/main/resources/people.json\"\n",
    "peopleDF=spark.read.json(path)\n",
    "\n",
    "peopleDF.printSchema()\n",
    "\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "teenagerNamesDF=spark.sql(\"select name from people where age between 13 and 19\")\n",
    "teenagerNamesDF.show()\n",
    "\n",
    "jsonString=['{\"name\":\"yin\",\"adress\":{\"city\":\"Beijing\",\"state\":\"xizhimen\"}}']\n",
    "otehpeopleRDD=sc.parallelize(jsonString)\n",
    "otherpeople=spark.read.json(otehpeopleRDD)\n",
    "otherpeople.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JDBC To Other Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "# Loading data from a JDBC source\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Saving data to a JDBC source\n",
    "jdbcDF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .save()\n",
    "\n",
    "jdbcDF2.write \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分布式SQL引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thrift JDBC / ODBC服务器：\n",
    "./bin/beeline\n",
    "beeline> !connect jdbc:hive2://localhost:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- subject_code: string (nullable = true)\n",
      " |-- dept_code: string (nullable = true)\n",
      " |-- group_organ_code: string (nullable = true)\n",
      " |-- money: string (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- large_area: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- data_type: string (nullable = true)\n",
      " |-- subject_detail: string (nullable = true)\n",
      " |-- subject_type: string (nullable = true)\n",
      " |-- subject_detail_type: string (nullable = true)\n",
      " |-- subject_type_8: string (nullable = true)\n",
      " |-- subject_type_15: string (nullable = true)\n",
      " |-- total_subject_type: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- create_by: string (nullable = true)\n",
      " |-- create_datetime: string (nullable = true)\n",
      " |-- update_by: string (nullable = true)\n",
      " |-- update_datetime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "path=\"../../spark-test.csv\"\n",
    "financeDF=spark.read.csv(path,header=True)\n",
    "financeDF.printSchema()\n",
    "\n",
    "# financeDF.createGlobalTempView(\"finance\")\n",
    "#df.registerTempTable(\"people\");  --注册临时表\n",
    "# teenagerNamesDF=spark.sql(\"select * from global_temp.finance limit 10 \")\n",
    "# teenagerNamesDF.cache(\"fa\")\n",
    "# teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financeDF.createGlobalTempView(\"finance\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--中文  spark.sql(\"\"\"select date as `是` from global_temp.finance limit 10\"\"\")    --使用限定符 ``     来写中文别名   as `中文`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|      中文|\n",
      "+--------+\n",
      "|2016/4/1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagerNamesDF=spark.sql(\"\"\"select date as `中文` from global_temp.finance limit 1\"\"\").show()      \n",
    "# teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+\n",
      "|    date|data_type|               收入|\n",
      "+--------+---------+-----------------+\n",
      "|2016/4/1|       实际|       1310081.17|\n",
      "|2016/5/1|       实际|5463629.550000001|\n",
      "|2016/6/1|       实际|             40.0|\n",
      "+--------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagerNamesDF=spark.sql(\"\"\" select date  ,substring(data_type,5) as data_type,\n",
    "sum( case when subject_type='收入' then money else 0 end )  `收入`\n",
    "from global_temp.finance \n",
    "group by date,data_type\n",
    "order by date,data_type\n",
    "  \"\"\")\n",
    "teenagerNamesDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------------+\n",
      "|    date|data_type|               收入|\n",
      "+--------+---------+-----------------+\n",
      "|2016/4/1|       实际|       1310081.17|\n",
      "|2016/5/1|       实际|5463629.550000001|\n",
      "|2016/6/1|       实际|             40.0|\n",
      "+--------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagerNamesDF.createGlobalTempView(\"finance_basic\")\n",
    "fbDF=spark.sql(\"\"\"select * from global_temp.finance_basic\"\"\")\n",
    "fbDF.show()\n",
    "\n",
    "# teenagerNamesDF.createOrReplaceTempView(\"finance_basic\")\n",
    "# fbDF=spark.sql(\"\"\"select * from finance_basic\"\"\")\n",
    "# fbDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+\n",
      "|    date|data_type|        收入|\n",
      "+--------+---------+----------+\n",
      "|2016/4/1|       实际|1310081.17|\n",
      "+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fbDF=spark.sql(\"\"\"select * from global_temp.finance_basic limit 1\"\"\")\n",
    "fbDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+-------------------------------------+----------------------------------------+\n",
      "|    date|    date|date|trunc(CAST(date AS DATE), yyyy-MM-dd)|trunc(CAST(2018-01-03 AS DATE), YYYY-MM)|\n",
      "+--------+--------+----+-------------------------------------+----------------------------------------+\n",
      "|2016/4/1|2016/4/1|null|                                 null|                                    null|\n",
      "|2016/4/1|2016/4/1|null|                                 null|                                    null|\n",
      "+--------+--------+----+-------------------------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select t.date,CAST(t.date as string) ,CAST(t.date as DATE)-- ,datediff(\"2009-03-01\", \"2009-02-27\")\n",
    "--,datediff(\"2018-01-01\",CAST(t.date as string) )\n",
    ",trunc(date,\"yyyy-MM-dd\"),trunc(\"2018-01-03\",\"YYYY-MM\")\n",
    "from global_temp.finance t limit 2 \"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.sql 支持 Hive sql的大部分函数  \n",
    "spark.sql 时间 认 \"-\" 不认\"/\"  ,需要先把 / 替换为 -\n",
    "    from_unixtime(unix_timestamp(BIRTHDAY,'yyyy/MM/dd HH:mm:ss'),'yyyy-MM'),\n",
    "    date_format(cast(regexp_replace(BIRTHDAY,'/','-') as date),'yyyy-MM'),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, 去年同期月份: string, 财年: string, subject_type: string, 钱: string]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\" select date \n",
    "--,to_date('2018-01-03') ,cast(regexp_replace(date,\"/\",\"-\") as date ),CURRENT_DATE\n",
    "--,date_format(cast(regexp_replace(date,\"/\",\"-\") as date),\"yyyy-MM\") as `月份`\n",
    "--,from_unixtime(unix_timestamp(date,'yyyy/MM/dd'),'yyyy-MM')\n",
    "--,trunc(regexp_replace(date,\"/\",\"-\") ,\"YYYY\"),trunc(regexp_replace(date,\"/\",\"-\") ,\"MM\")\n",
    "--,add_months(regexp_replace(date,\"/\",\"-\"),-12)\n",
    ",date_format(add_months(regexp_replace(date,\"/\",\"-\"),-12),\"yyyy-MM\") as `去年同期月份`\n",
    ",date_format(add_months(regexp_replace(date,\"/\",\"-\"),-3),\"yyyy\")  as `财年`\n",
    ",subject_type,case when subject_type='收入' then money else 0 end  as `钱`\n",
    "from global_temp.finance where subject_type='收入' limit 5 \"\"\") #.show()\n",
    "\n",
    "# teenagerNamesDF1=spark.sql(\"\"\" select date from global_temp.finance  limit 10 \"\"\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`a1.\\u533a\\u57df`' given input columns: [company_name, subject_detail, create_datetime, id, money, date, update_datetime, subject_type_15, subject_detail, subject_detail_type, subject_type_8, data_type, date, update_datetime, large_area, subject_detail_type, area, data_type, total_subject_type, area, subject_type_8, dept, money, update_by, subject_type_15, update_by, create_by, id, group_organ_code, items, total_subject_type, create_by, dept, subject_code, subject_type, subject_code, items, subject_type, group_organ_code, dept_code, company_name, dept_code, create_datetime, large_area]; line 4 pos 21;\\n'Project [*]\\n+- 'Filter ('bb.\\u533a\\u57df = \\u5317\\u4eac)\\n   +- 'SubqueryAlias bb\\n      +- 'Project ['a1.\\u533a\\u57df, 'a1.\\u6570\\u636e\\u7c7b\\u578b2]\\n         +- 'Join LeftOuter, ((('a1.\\u533a\\u57df = 'a0.\\u533a\\u57df) && ('a1.\\u6570\\u636e\\u7c7b\\u578b2 = 'a0.\\u6570\\u636e\\u7c7b\\u578b2)) && ('a1.\\u53bb\\u5e74\\u540c\\u671f\\u6708\\u4efd = 'a0.\\u6708\\u4efd))\\n            :- SubqueryAlias a1\\n            :  +- SubqueryAlias aa\\n            :     +- Project [id#0, date#1, subject_code#2, dept_code#3, group_organ_code#4, money#5, area#6, large_area#7, dept#8, items#9, data_type#10, subject_detail#11, subject_type#12, subject_detail_type#13, subject_type_8#14, subject_type_15#15, total_subject_type#16, company_name#17, create_by#18, create_datetime#19, update_by#20, update_datetime#21]\\n            :        +- SubqueryAlias finance, `global_temp`.`finance`\\n            :           +- Relation[id#0,date#1,subject_code#2,dept_code#3,group_organ_code#4,money#5,area#6,large_area#7,dept#8,items#9,data_type#10,subject_detail#11,subject_type#12,subject_detail_type#13,subject_type_8#14,subject_type_15#15,total_subject_type#16,company_name#17,create_by#18,create_datetime#19,update_by#20,update_datetime#21] csv\\n            +- SubqueryAlias a0\\n               +- SubqueryAlias aa\\n                  +- Project [id#8026, date#8027, subject_code#8028, dept_code#8029, group_organ_code#8030, money#8031, area#8032, large_area#8033, dept#8034, items#8035, data_type#8036, subject_detail#8037, subject_type#8038, subject_detail_type#8039, subject_type_8#8040, subject_type_15#8041, total_subject_type#8042, company_name#8043, create_by#8044, create_datetime#8045, update_by#8046, update_datetime#8047]\\n                     +- SubqueryAlias finance, `global_temp`.`finance`\\n                        +- Relation[id#8026,date#8027,subject_code#8028,dept_code#8029,group_organ_code#8030,money#8031,area#8032,large_area#8033,dept#8034,items#8035,data_type#8036,subject_detail#8037,subject_type#8038,subject_detail_type#8039,subject_type_8#8040,subject_type_15#8041,total_subject_type#8042,company_name#8043,create_by#8044,create_datetime#8045,update_by#8046,update_datetime#8047] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-520-c18e66feb1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   left join aa a0 on a1.`区域`=a0.`区域` and a1.`数据类型2`=a0.`数据类型2` and a1.`去年同期月份`=a0.`月份` )\n\u001b[1;32m      6\u001b[0m \u001b[0mselect\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbb\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;31m区\u001b[0m\u001b[0;31m域\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"北京\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/root/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/Downloads/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`a1.\\u533a\\u57df`' given input columns: [company_name, subject_detail, create_datetime, id, money, date, update_datetime, subject_type_15, subject_detail, subject_detail_type, subject_type_8, data_type, date, update_datetime, large_area, subject_detail_type, area, data_type, total_subject_type, area, subject_type_8, dept, money, update_by, subject_type_15, update_by, create_by, id, group_organ_code, items, total_subject_type, create_by, dept, subject_code, subject_type, subject_code, items, subject_type, group_organ_code, dept_code, company_name, dept_code, create_datetime, large_area]; line 4 pos 21;\\n'Project [*]\\n+- 'Filter ('bb.\\u533a\\u57df = \\u5317\\u4eac)\\n   +- 'SubqueryAlias bb\\n      +- 'Project ['a1.\\u533a\\u57df, 'a1.\\u6570\\u636e\\u7c7b\\u578b2]\\n         +- 'Join LeftOuter, ((('a1.\\u533a\\u57df = 'a0.\\u533a\\u57df) && ('a1.\\u6570\\u636e\\u7c7b\\u578b2 = 'a0.\\u6570\\u636e\\u7c7b\\u578b2)) && ('a1.\\u53bb\\u5e74\\u540c\\u671f\\u6708\\u4efd = 'a0.\\u6708\\u4efd))\\n            :- SubqueryAlias a1\\n            :  +- SubqueryAlias aa\\n            :     +- Project [id#0, date#1, subject_code#2, dept_code#3, group_organ_code#4, money#5, area#6, large_area#7, dept#8, items#9, data_type#10, subject_detail#11, subject_type#12, subject_detail_type#13, subject_type_8#14, subject_type_15#15, total_subject_type#16, company_name#17, create_by#18, create_datetime#19, update_by#20, update_datetime#21]\\n            :        +- SubqueryAlias finance, `global_temp`.`finance`\\n            :           +- Relation[id#0,date#1,subject_code#2,dept_code#3,group_organ_code#4,money#5,area#6,large_area#7,dept#8,items#9,data_type#10,subject_detail#11,subject_type#12,subject_detail_type#13,subject_type_8#14,subject_type_15#15,total_subject_type#16,company_name#17,create_by#18,create_datetime#19,update_by#20,update_datetime#21] csv\\n            +- SubqueryAlias a0\\n               +- SubqueryAlias aa\\n                  +- Project [id#8026, date#8027, subject_code#8028, dept_code#8029, group_organ_code#8030, money#8031, area#8032, large_area#8033, dept#8034, items#8035, data_type#8036, subject_detail#8037, subject_type#8038, subject_detail_type#8039, subject_type_8#8040, subject_type_15#8041, total_subject_type#8042, company_name#8043, create_by#8044, create_datetime#8045, update_by#8046, update_datetime#8047]\\n                     +- SubqueryAlias finance, `global_temp`.`finance`\\n                        +- Relation[id#8026,date#8027,subject_code#8028,dept_code#8029,group_organ_code#8030,money#8031,area#8032,large_area#8033,dept#8034,items#8035,data_type#8036,subject_detail#8037,subject_type#8038,subject_detail_type#8039,subject_type_8#8040,subject_type_15#8041,total_subject_type#8042,company_name#8043,create_by#8044,create_datetime#8045,update_by#8046,update_datetime#8047] csv\\n\""
     ]
    }
   ],
   "source": [
    "#支持 with \n",
    "spark.sql(\"\"\" with aa as (select * from global_temp.finance  )\n",
    "  ,bb as ( select a1.`区域`,a1.`数据类型2`\n",
    "  from aa a1 \n",
    "  left join aa a0 on a1.`区域`=a0.`区域` and a1.`数据类型2`=a0.`数据类型2` and a1.`去年同期月份`=a0.`月份` )\n",
    "select * from bb where bb.`区域`=\"北京\"  \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+-----+-----+----------+--------+\n",
      "|  财年|     月份| 区域|  数据类型|数据类型2|是否一体化|      体检收入|去年同期体检收入|\n",
      "+----+-------+---+------+-----+-----+----------+--------+\n",
      "|2016|2016-05| 北京|2016实际|   实际|  一体化|5268518.68|    null|\n",
      "|2016|2016-04| 北京|2016实际|   实际|  一体化|    1220.0|    null|\n",
      "+----+-------+---+------+-----+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 报错了先查看 表是否写的正确   ,注意 别名 为中文的字段，引用时也要加 限定符 ``  ex:select * from aa where aa.`区域`=\"北京\"\n",
    "teenagerNamesDF=spark.sql(\"\"\" with aa as ( select date_format(add_months(regexp_replace(date,\"/\",\"-\"),-3),\"yyyy\")  as `财年`\n",
    ",date_format(cast(regexp_replace(date,\"/\",\"-\") as date),\"yyyy-MM\") as `月份`\n",
    ",date_format(add_months(regexp_replace(date,\"/\",\"-\"),-12),\"yyyy-MM\") as `去年同期月份`\n",
    ",date `日期`,area `区域`,data_type `数据类型`,substring(data_type,5) `数据类型2`,\n",
    "sum( case when subject_type='收入' then money else 0 end ) as `全收入`,\n",
    "sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ) as `齿科双算`, -- 60011108收入 \n",
    "sum( case when subject_detail_type='体检收入' then money else 0 end ) as `体检收入`,\n",
    "sum( case when subject_code='64010201' or subject_code='64010202' then money else 0 end ) as  `变动成本1` ,   --64010201 and 64010202\n",
    "sum( case when subject_detail_type='疾病检测收入' then money else 0 end ) as `疾病检测收入`,\n",
    "sum( case when subject_detail_type='齿科收入' then money else 0 end ) as `齿科收入`,\n",
    "sum( case when subject_code='64010221' then money else 0 end ) as `变动成本3`   ,  --64010221\n",
    "sum( case when subject_detail_type='门诊收入' then money else 0 end ) as `门诊收入`,\n",
    "sum( case when subject_code='64010222' then money else 0 end ) as `变动成本4`  , --64010222\n",
    "sum( case when subject_detail_type='医疗管理收入' then money else 0 end ) as `医疗管理收入`,\n",
    "sum( case when subject_code='64010211' then money else 0 end ) as `变动成本5`,   --64010211\n",
    "sum( case when subject_detail_type='销售商品收入' then money else 0 end ) as `销售商品收入`,\n",
    "sum( case when subject_detail_type='其他收入' then money else 0 end ) as `其他收入`,\n",
    "sum( case when subject_detail_type='落关联成本' then money else 0 end ) as `落关联成本`,\n",
    "sum( case when subject_type='成本费用' then money else 0 end ) as `成本费用`, \n",
    "nvl( sum( case when subject_type='收入' then money else 0 end ),0)-nvl(sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ),0) as `收入`,\n",
    "nvl( sum( case when subject_type='收入' then money else 0 end ),0)-nvl(sum( case when subject_type='成本费用' then money else 0 end ) ,0) as `税前利润`,\n",
    "case when area in ('西康','华检','元化医疗','健维管理','臻景','香港','BVI') then '非一体化'  else '一体化' end as `是否一体化`\n",
    "from global_temp.finance\n",
    "group by date_format(add_months(regexp_replace(date,\"/\",\"-\"),-3),\"yyyy\")\n",
    "    ,date_format(cast(regexp_replace(date,\"/\",\"-\") as date),\"yyyy-MM\")\n",
    "    ,date_format(add_months(regexp_replace(date,\"/\",\"-\"),-12),\"yyyy-MM\")\n",
    "    ,date ,area ,data_type,substring(data_type,5)\n",
    "    ,case when area in ('西康','华检','元化医疗','健维管理','臻景','香港','BVI') then '非一体化' else '一体化' end \n",
    ")\n",
    ",a1 as ( --今年和去年同期 处理为一条\n",
    "select a1.`财年`,a1.`月份`,a1.`区域`,a1.`数据类型`,a1.`数据类型2`,a1.`是否一体化`,a1.`体检收入`,a0.`体检收入`  as `去年同期体检收入`\n",
    "from aa a1\n",
    "left join aa a0 on a1.`区域`=a0.`区域` and a1.`数据类型2`=a0.`数据类型2` and a1.`去年同期月份`=a0.`月份`\n",
    ")\n",
    "--select * from aa where aa.`区域`=\"北京\"\n",
    "select * from a1 where a1.`区域`=\"北京\"\n",
    "\"\"\")\n",
    "\n",
    "teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",a1 as ( --今年和去年同期 处理为一条\n",
    "select \n",
    "a1.财年,a1.月份,a1.区域,a1.数据类型,a1.数据类型2,a1.是否一体化, \n",
    "a1.体检收入,a1.变动成本1,a1.疾病检测收入,a1.齿科收入,a1.变动成本3,a1.门诊收入,a1.变动成本4,a1.医疗管理收入,\n",
    "a1.变动成本5,a1.销售商品收入,a1.其他收入,a1.落关联成本,a1.收入,\n",
    "a0.体检收入  as 去年同期体检收入,  a0.变动成本1 as 去年同期变动成本1 ,  a0.疾病检测收入 as 去年同期疾病检测收入,\n",
    "a0.齿科收入 as 去年同期齿科收入,    a0.变动成本3 as 去年同期变动成本3,   a0.门诊收入 as 去年同期门诊收入,    \n",
    "a0.变动成本4  as 去年同期变动成本4,   a0.医疗管理收入  as 去年同期医疗管理收入,a0.变动成本5 as 去年同期变动成本5,   \n",
    "a1.销售商品收入 as 去年同期销售商品收入,a1.其他收入 as 去年同期其他收入,    a1.落关联成本 as 去年同期落关联成本,  \n",
    "a1.收入 as 去年同期收入\n",
    "from a0 a1\n",
    "left join a0 on a1.区域=a0.区域 and a1.数据类型2=a0.数据类型2 and a1.去年同期月份=a0.月份\n",
    ")\n",
    "select * from a1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+-----+---------+----+\n",
      "|      日期| 区域|  数据类型|数据类型2|      全收入|齿科双算|\n",
      "+--------+---+------+-----+---------+----+\n",
      "|2016/5/1| 江阴|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/4/1| 江阴|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 广州|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 杭州|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/4/1| 常州|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/6/1| 电商|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/4/1| 潍坊|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 芜湖|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 深圳|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 佛山|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 沈阳|2016实际|   实际|150239.03| 0.0|\n",
      "|2016/4/1| 天津|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 天津|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 苏州|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/6/1| 总部|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 重庆|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| JT|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/6/1| 杭州|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/5/1| 威海|2016实际|   实际|      0.0| 0.0|\n",
      "|2016/6/1| 武汉|2016实际|   实际|     40.0| 0.0|\n",
      "+--------+---+------+-----+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\" select date `日期` ,substring(data_type,5) as data_type,\n",
    "# sum( case when subject_type='收入' then money else 0 end )  `收入`\n",
    "# ,sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ) as `齿科双算` \n",
    "# from global_temp.finance \n",
    "# group by date ,area ,data_type,substring(data_type,5)\n",
    "# order by date,data_type\n",
    "#   \"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"select date `日期`,area `区域`,data_type `数据类型`,substring(data_type,5) `数据类型2`,\n",
    "sum( case when subject_type='收入' then money else 0 end ) as `全收入`,\n",
    "sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ) as `齿科双算`\n",
    "from global_temp.finance\n",
    "group by date ,area ,data_type,substring(data_type,5)\n",
    "    \"\"\").show()\n",
    "\n",
    "\n",
    "# spark.sql(\"\"\"select date `日期`,area `区域`,data_type `数据类型`,substring(data_type,5) `数据类型2`,\n",
    "# sum( case when subject_type='收入' then money else 0 end ) as `全收入`,\n",
    "# sum( case when subject_type='收入' and subject_code='60011108' then money else 0 end ) as `齿科双算`\n",
    "# from global_temp.finace\n",
    "# group by date ,area ,data_type,substring(data_type,5)\n",
    "#     \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
    ">>> df.select(date_add(df.d, 1).alias('d')).collect()\n",
    "[Row(d=datetime.date(2015, 4, 9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
