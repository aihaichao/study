{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark sql study"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=sc.parallelize([\"pan\",\"i like pan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pan'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print i*i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#导入spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext,Row"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#当不能引入hive依赖时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext,Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlctx=sqlContextl(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hivectx=HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入门"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 创建基本的sparksession，只需要使用sparksession.builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "根据JSON文件的内容创建一个DataFrame："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "路径 本次是项目所在的路径下开始找 ，（或者 启动服务的目录开始找） \n",
    "file:/root/Downloads/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(\"../../examples/src/main/resources/people.json\")\n",
    "#df=spark.read.json(\"./examples/src/main/resources/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+\n",
      "|   name| age|(age + 1)|\n",
      "+-------+----+---------+\n",
      "|Michael|null|     null|\n",
      "|   Andy|  30|       31|\n",
      "| Justin|  19|       20|\n",
      "+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'],df['age'],df['age']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age']>21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('age').count().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SparkSession上的sql函数使应用程序能够以编程方式运行SQL查询，并将结果作为DataFrame返回。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Register the DataFrame as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF=spark.sql(\"select * from people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Global temporary view is cross-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"select * from global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集(Dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Inferring the Schema Using Reflection   使用反射推理schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines=sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts=lines.map(lambda l:l.split(\",\"))\n",
    "people=parts.map(lambda p: Row(name=p[0],age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople =spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "#spark.sql(\"select * from people\").show()\n",
    "#SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers=spark.sql(\"select name from people where age>=13 and age <=19\")\n",
    "teenagers.show()\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "\n",
    "teenNames=teenagers.rdd.map(lambda p: \"Name: \" +p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "\n",
    "# for name in teenagers:\n",
    "#     print(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Programmatically Specifying the Schema 已编程方式指定schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc=spark.sparkContext\n",
    "\n",
    "lines=sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts=lines.map(lambda l:l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people=parts.map(lambda p:(p[0],p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString=\"name age\"\n",
    "\n",
    "fields=[StructField(field_name,StringType(),True) for field_name in schemaString.split()]\n",
    "schema=StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople=spark.createDataFrame(people,schema)\n",
    "\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "#results=spark.sql(\"select name from people\")\n",
    "results=spark.sql(\"select * from people\")\n",
    "results.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\",\"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "直接在文件上运行SQL     \"   ``  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from parquet.`examples/src/main/resources/users.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "保存到持久表    saveAsTable将物化saveAsTable的内容并创建指向Hive Metastore中的数据的指针"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "+------------------+----+\n",
      "|            adress|name|\n",
      "+------------------+----+\n",
      "|[Beijing,xizhimen]| yin|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "path=\"examples/src/main/resources/people.json\"\n",
    "peopleDF=spark.read.json(path)\n",
    "\n",
    "peopleDF.printSchema()\n",
    "\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "teenagerNamesDF=spark.sql(\"select name from people where age between 13 and 19\")\n",
    "teenagerNamesDF.show()\n",
    "\n",
    "jsonString=['{\"name\":\"yin\",\"adress\":{\"city\":\"Beijing\",\"state\":\"xizhimen\"}}']\n",
    "otehpeopleRDD=sc.parallelize(jsonString)\n",
    "otherpeople=spark.read.json(otehpeopleRDD)\n",
    "otherpeople.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JDBC To Other Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "# Loading data from a JDBC source\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "# Saving data to a JDBC source\n",
    "jdbcDF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .save()\n",
    "\n",
    "jdbcDF2.write \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
